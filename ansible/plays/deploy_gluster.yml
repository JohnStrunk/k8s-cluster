---

# Deploys Gluster onto k8s nodes

- hosts: workers
  become: true
  vars:
    gluster_disks: [vdb]
  tasks:
    # Format & mount extra disks
    - name: Create mountpoint for bricks
      file:
        state: directory
        name: /mnt/bricks/{{ item }}
      with_items: "{{ gluster_disks }}"
    - name: Make sure bricks are not mounted
      mount:
        path: /mnt/bricks/{{ item }}
        state: unmounted
      with_items: "{{ gluster_disks }}"
    - name: Format bricks with xfs
      filesystem:
        fstype: xfs
        dev: /dev/{{ item }}
        force: yes
        opts: -i size=512
      with_items: "{{ gluster_disks }}"
    - name: Mount {{ item }}
      mount:
        path: /mnt/bricks/{{ item }}
        src: /dev/{{ item }}
        fstype: xfs
        state: mounted
      with_items: "{{ gluster_disks }}"
    - name: Create root directory on brick
      file:
        state: directory
        name: /mnt/bricks/{{ item }}/b
      with_items: "{{ gluster_disks }}"

- hosts: master
  vars:
    gluster_nodes: [worker0, worker1, worker2]
    ns: glusterfs
  tasks:
    # Namespace for gluster stuff
    - name: Check for {{ ns }} namespace
      command: kubectl get namespace {{ ns }}
      ignore_errors: true
      register: ns_check
    - name: Create namespace - {{ ns }}
      command: kubectl create namespace {{ ns }}
      when: ns_check.rc != 0

    # Deploy gluster servers
    - name: Label nodes to receive Gluster servers
      command: kubectl label nodes {{ item }} --overwrite storagenode=glusterfs
      with_items: "{{ gluster_nodes }}"
    - name: Copy Gluster daemonset template
      template:
        src: ../templates/glusterfs-daemonset.yaml.j2
        dest: /tmp/glusterfs-daemonset.yaml
    - name: Deploy Gluster daemonset
      command: kubectl create -n {{ ns }} -f /tmp/glusterfs-daemonset.yaml
    - name: wait for gluster pods to be ready
      shell: test "$(kubectl -n glusterfs get ds/glusterfs -o jsonpath='{.status.numberReady}')" = "$(kubectl -n glusterfs get ds/glusterfs -o jsonpath='{.status.desiredNumberScheduled}')"
      register: task_result
      until: task_result.rc == 0
      delay: 10
      retries: 30
      changed_when: false

    # Create endpoints and service
    - name: Copy endpoint script
      template:
        src: ../templates/glusterfs-svc.sh.j2
        dest: /tmp/glusterfs-svc.sh
        mode: 0755
    - name: Create endpoint and svc
      command: /tmp/glusterfs-svc.sh

    # Make a big volume
#    - name: Get a gluster pod IPs
#      command: kubectl -n {{ ns }} get po -l glusterfs-node=pod -o jsonpath='{.items[*].status.podIP}'
#      register: gpodips
    - name: Get a gluster pod name
      command: kubectl -n {{ ns }} get po -l glusterfs-node=pod -o jsonpath='{.items[0].metadata.name}'
      register: gpodname
    - name: Peer probe
      command: kubectl -n {{ ns }} exec {{ gpodname.stdout }} -- gluster peer probe {{ item }}
      with_items: "{{ gluster_nodes }}"
    - name: Create volume
      command: "kubectl -n {{ ns }} exec {{ gpodname.stdout }} -- gluster volume create vol1 replica 3 arbiter 1 {{ gluster_nodes | map('regex_replace', '(.+)', '\1:/mnt/bricks/vdb/b') | join (' ') }}"
    - name: Start volume
      command: kubectl -n {{ ns }} exec {{ gpodname.stdout }} -- gluster volume start vol1
#    - name: Copy PV templates for big volume
#      template:
#        src: ../templates/pv_vol1.yml
#        dest: /tmp/pv_vol1.yml
#    - name: Create PV for big volume
#      command: kubectl -n {{ ns }} create -f /tmp/pv_vol1.yml
